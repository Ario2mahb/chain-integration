groups:
- name: testnet-alert
  rules:

  # Alert for any instance that is unreachable for >5 minutes.
  - alert: InstanceDown
    expr: up == 0
    for: 5m
    labels:
      severity: 'page'
    annotations:
      title: 'Instance {{ $labels.instance }} down'
      summary: "Instance {{ $labels.instance }} down"
      description: "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes."
      link: '/graph?g0.range_input=1h&g0.expr=up%20%3D%3D%200&g0.tab=1'
  - alert: chain_main_consensus_height_not_increase
    expr: |
        changes(tendermint_consensus_height{}[5m]) == 0
    annotations:
      title: '{{ $labels.env }} {{ $labels.job }} {{ $labels.instance }} stops generating block'
      description: "The height of tendermint block in {{ $labels.instance }} increases fewer than 1 in 5 minutes."
      link: '/graph?g0.range_input=1h&g0.expr=increase(tendermint_consensus_height%7B%7D%5B5m%5D)%3C1&g0.tab=1'
    for: 3m
    labels:
      severity: 'critical'
  - alert: chain_main_behind
    for: 1m
    expr: tendermint_consensus_height - ignoring (instance) group_left max without (instance)(tendermint_consensus_height) <= -5
    annotations:
      summary: "NODE seems behind'{{ $labels.job }}' on '{{ $labels.instance }}'"
      severity: "HIGH"
      description: "'height of {{ $labels.job }}' on {{ $labels.instance }} is behind our best height by {{ $value }} for longer than 1minute."
      link: '/graph?g0.range_input=1h&g0.expr=tendermint_consensus_height%20-%20ignoring%20(instance)%20group_left%20max%20without%20(instance)(tendermint_consensus_height)%20<%3D%20-5&g0.tab=1'
  - alert: node_reboot
    expr: changes(node_boot_time_seconds[10m]) > 1
    annotations:
      summary: "NODE has rebooted'{{ $labels.job }}' on '{{ $labels.instance }}'"
      severity: "HIGH"
      description: "It looks like {{ $labels.instance }}' has rebooted. Was that intentional ?"
      link: '/graph?g0.range_input=1h&g0.expr=changes(node_boot_time_seconds%5B10m%5D)%20>%201&g0.tab=1'
  - alert: validator_connection
    for: 30s
    expr: -tendermint_p2p_peers + ignoring (instance) group_left count without (instance)(tendermint_consensus_height) -1 > 0
    annotations:
      summary: '{{ $labels.instance }} of job {{ $labels.job }} is not connected to all other nodes.'
      severity: "HIGH"
      description: "{{ $labels.instance }} has a peering problem with other private nodes. Nodes missing: {{$value}}."
      link: '/graph?g0.range_input=1h&g0.expr=-tendermint_p2p_peers%20%2B%20ignoring%20(instance)%20group_left%20count%20without%20(instance)(tendermint_consensus_height)%20-1%20>%200&g0.tab=1'
  - alert: network_errors
    for: 30s
    expr: sum(rate(node_network_receive_errs_total[5m])) > 1
    annotations:
      summary: '{{ $labels.instance }} of job {{ $labels.job }} has network receive errors.'
      severity: "HIGH"
      description: "{{ $labels.instance }} has networking issues. Package error rate: {{$value}}."
      link: '/graph?g0.range_input=1h&g0.expr=sum(rate(node_network_receive_errs_total%5B5m%5D))%20>%201&g0.tab=1'
  - alert: socket_opens
    for: 5m
    expr: delta(node_sockstat_TCP_alloc[1m]) > 200
    annotations:
      summary: '{{ $labels.instance }} allocates TCP sockets at a very high rate.'
      severity: "HIGH"
      description: "{{ $labels.instance }} allocates very many TCP sockets. TCP sockets / second: {{$value}}."
      link: '/graph?g0.range_input=1h&g0.expr=delta(node_sockstat_TCP_alloc%5B1m%5D)%20>%20200&g0.tab=1'
  - alert: disk_will_fill_in_8_hours
    for: 5m
    expr: predict_linear(node_filesystem_free_bytes{mountpoint="/"}[1h], 8*3600) < 0
    annotations:
      summary: "DISK SPACE FULL IN 8 HOURS: NODE '{{ $labels.instance }}'"
      severity: "HIGH"
      description: "{{ $labels.instance }} is writing a lot."
      link: '/graph?g0.range_input=1h&g0.expr=predict_linear(node_filesystem_free_bytes%7Bmountpoint%3D"%2F"%7D%5B1h%5D%2C%208*3600)%20<%200&g0.tab=1'
  - alert: Tendermint_Node_CPU_TOO_HIGH_LOAD
    annotations:
      title: '{{ $labels.env }} {{ $labels.job }} {{ $labels.instance }} has high CPU usage'
      description: "{{ $labels.instance }} exceeds 80 CPU Used % and reach at {{ $value }}% in [5m] avg."
      link: '/graph?g0.range_input=1h&g0.expr=100%20-%20(avg%20by%20(instance%2C%20job%2C%20ip%2C%20env)%20(irate(node_cpu_seconds_total%7Bmode%3D%22idle%22%7D%5B5m%5D))%20*%20100)%3E80&g0.tab=1'
    expr: |
        100 - (avg by (instance, job, ip, env) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
    for: 3m
    labels:
      severity: 'warning'
  - alert: Tendermint_Node_MEM_TOO_HIGH_LOAD
    annotations:
      title: '{{ $labels.env }} {{ $labels.job }} {{ $labels.instance }} memory is almost full.'
      description: "{{ $labels.instance }} exceeds 85 MEMORY Used % and reach at {{ $value }}%."
      link: '/graph?g0.range_input=1h&g0.expr=(sum%20by%20(instance%2C%20job%2C%20ip%2C%20env)%20(node_memory_MemTotal_bytes)%20-%20sum%20by%20(instance%2C%20job%2C%20ip%2C%20env)%20(node_memory_MemFree_bytes%20%2B%20node_memory_Buffers_bytes%20%2B%20node_memory_Cached_bytes)%20)%20%2F%20sum%20by%20(instance%2C%20job%2C%20ip%2C%20env)(node_memory_MemTotal_bytes)%20*%20100%3E85&g0.tab=1'
    expr: |
        (sum by (instance, job, ip, env) (node_memory_MemTotal_bytes) - sum by (instance, job, ip, env) (node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes) ) / sum by (instance, job, ip, env)(node_memory_MemTotal_bytes) * 100 > 85
    for: 3m
    labels:
      severity: 'warning'
  - alert: Tendermint_Node_STORAGE_TOO_HIGH_LOAD
    annotations:
      title: '{{ $labels.env }} {{ $labels.job }} {{ $labels.instance }} DISK STORAGE is almost full'
      description: "{{ $labels.instance }} exceeds 85 STORAGE Used % and reach at {{ $value }}%."
      link: '/graph?g0.range_input=1h&g0.expr=(node_filesystem_size_bytes%7Bmountpoint%3D"%2Fmnt"%7D%20-%20node_filesystem_free_bytes%7Bmountpoint%3D"%2Fmnt"%7D)%20%2F%20node_filesystem_size_bytes%7Bmountpoint%3D"%2Fmnt"%7D%20*%20100%20>%2085&g0.tab=1'
    expr: |
        (node_filesystem_size_bytes{mountpoint="/mnt"} - node_filesystem_free_bytes{mountpoint="/mnt"}) / node_filesystem_size_bytes{mountpoint="/mnt"} * 100 > 85
    for: 3m
    labels:
      severity: 'warning'